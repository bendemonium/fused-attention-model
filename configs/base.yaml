# configs/fuseformer_babylm.yaml
# Configuration for training FuseFormer on BabyLM tokenized data

# =========================
# Dataset Configuration
# =========================
dataset_name: "bendemonium/babylm25_tokens"
tokenizer_name: "gpt2"
train_split: "train"
dev_split: "dev"  # Will fallback to validation if dev doesn't exist
streaming: false

# =========================
# Model Architecture 
# =========================
# Core architecture - REDUCED for memory and stability
vocab_size: 50258     # GPT-2 vocab size + 1 for mask token
d_model: 256          # REDUCED from 768 for memory
num_heads: 4          # REDUCED from 12 for memory
num_layers: 4         # REDUCED from 12 for memory
d_ff: 512            # REDUCED (4 * d_model)
max_position_embeddings: 1024

# Regularization
dropout: 0.1
attn_dropout: 0.1
layer_norm_eps: 1e-5
max_grad_norm: 1.0

# =========================
# Hyperbolic-Specific Settings
# =========================
# Lorentz hyperboloid parameters - CONSERVATIVE
lorentz_spatial_dim: 32    # REDUCED from 64 for memory
lorentz_tau: 0.05          # LOWER for stability (was 0.1)
karcher_steps: 1           # Steps for Karcher mean computation

# Fusion parameters
alpha_warmup_steps: 2000   # LONGER warmup (was 1000)
fusion_init_alpha: 0.0     # Start with pure Euclidean (alpha=0)

# Numerical stability - AGGRESSIVE
use_fp32_hyperbolic: true  # Keep hyperbolic ops in FP32
distance_cap: 3.0          # LOWER cap (was 5.0)
norm_cap: 1.5              # LOWER cap (was 2.0)

# RoPE settings
rope_max_seq_len: 4096
use_rope: true

# =========================
# Training Configuration
# =========================
# Basic training - MEMORY OPTIMIZED
batch_size: 4              # DRASTICALLY REDUCED from 16
eval_batch_size: 8         # REDUCED from 32
epochs: 3
num_workers: 2             # REDUCED from 4

# Learning rate and optimization
lr: 3e-4
weight_decay: 0.01
adam_betas: [0.9, 0.999]
adam_eps: 1e-8
lr_scheduler: "cosine"     # cosine, linear, or constant
warmup_steps: 1000

# MLM settings
mlm_probability: 0.15

# Mixed precision - DISABLED for stability
amp_dtype: "fp32"          # CHANGED: No mixed precision at all
gradient_checkpointing: true  # Enable to save memory

# =========================
# Checkpoint & Logging
# =========================
output_dir: "./outputs/fuseformer_babylm"

# Milestone saving (every N million words)
milestones_words: ["1m", "2m", "5m", "10m", "20m", "30m", "40m", "50m", "60m", "70m", "80m", "90m", "100m"]
push_every_milestone: true

# HuggingFace Hub settings
hf_repo: "bendemonium/fuseformer-babylm"  # Your model repo
hf_branch_prefix: "words-"
hf_create_repo: true
hf_private: false
# hf_token: "your_token_here"  # Or set HF_TOKEN env var

# Weights & Biases
wandb_mode: "online"        # online, offline, or disabled
wandb_project: "fuseformer-babylm"
# wandb_entity: "bendemonium" # Your W&B username
wandb_run_name: "fuseformer-v2-stable"
wandb_log_every: 10

# =========================
# Debugging & Monitoring
# =========================
# Validation frequency
eval_every_steps: 500

# Early stopping for NaN detection
abort_on_nan_step_0: true
log_attention_stats: true
log_hyperbolic_stats: true

# Gradient monitoring
log_grad_norms: true
clip_grad_norm: true

# =========================
# Experimental Settings
# =========================
# For ablation studies
euclidean_only: false      # Set true to disable hyperbolic branch
hyperbolic_only: false     # Set true to disable Euclidean branch
progressive_fusion: true   # Gradually increase alpha during training

# Advanced numerical stability - ENHANCED
use_identity_attention_fallback: true  # For all-masked rows
enable_attention_debugging: true       # Extra logging for attention issues
safe_mode_steps: 2000                  # EXTENDED safe mode (was 100)

# Memory optimization
clear_cache_every_n_steps: 50          # Clear CUDA cache frequently