# configs/fuseformer_babylm.yaml
# Configuration for training FuseFormer on BabyLM tokenized data

# =========================
# Dataset Configuration
# =========================
dataset_name: "bendemonium/babylm25_tokens"
tokenizer_name: "gpt2"
train_split: "train"
dev_split: "dev"  # Will fallback to validation if dev doesn't exist
streaming: false

# =========================
# Model Architecture 
# =========================
# Core architecture
vocab_size: 50257  # GPT-2 vocab size
d_model: 768       # Hidden dimension
num_heads: 12      # Attention heads  
num_layers: 12     # Transformer layers
d_ff: 3072         # FFN dimension (4 * d_model)
max_position_embeddings: 1024

# Regularization
dropout: 0.1
attn_dropout: 0.1
layer_norm_eps: 1e-5
max_grad_norm: 1.0

# =========================
# Hyperbolic-Specific Settings
# =========================
# Lorentz hyperboloid parameters
lorentz_spatial_dim: 64    # Spatial dimension for hyperbolic space
lorentz_tau: 0.1           # Temperature parameter (start low for stability)
karcher_steps: 1           # Steps for Karcher mean computation

# Fusion parameters  
alpha_warmup_steps: 1000   # Steps to warm up alpha from 0 to learned value
fusion_init_alpha: 0.0    # Start with pure Euclidean (alpha=0)

# Numerical stability
use_fp32_hyperbolic: true  # Keep hyperbolic ops in FP32
distance_cap: 5.0          # Cap hyperbolic distances
norm_cap: 2.0              # Cap parameter norms before exp_map

# RoPE settings
rope_max_seq_len: 4096
use_rope: true

# =========================
# Training Configuration
# =========================
# Basic training
batch_size: 16             # Start smaller for stability
eval_batch_size: 32
epochs: 3
num_workers: 4

# Learning rate and optimization
lr: 3e-4
weight_decay: 0.01
adam_betas: [0.9, 0.999]
adam_eps: 1e-8
lr_scheduler: "cosine"     # cosine, linear, or constant
warmup_steps: 1000

# MLM settings
mlm_probability: 0.15

# Mixed precision (careful with hyperbolic ops)
amp_dtype: "fp16"          # Use fp16, but hyperbolic ops stay fp32

# =========================
# Checkpoint & Logging
# =========================
output_dir: "./outputs/fuseformer_babylm"

# Milestone saving (every N million words)
milestones_words: ["1m", "2m", "5m", "10m", "20m", "50m", "100m"]
push_every_milestone: true

# HuggingFace Hub settings
hf_repo: "bendemonium/fuseformer-babylm"  # Your model repo
hf_branch_prefix: "words-"
hf_create_repo: true
hf_private: false
# hf_token: "your_token_here"  # Or set HF_TOKEN env var

# Weights & Biases
wandb_mode: "online"        # online, offline, or disabled
wandb_project: "fuseformer-babylm"
wandb_entity: "bendemonium" # Your W&B username
wandb_run_name: "fuseformer-v1"
wandb_log_every: 10

# =========================
# Debugging & Monitoring
# =========================
# Validation frequency
eval_every_steps: 500

# Early stopping for NaN detection
abort_on_nan_step_0: true
log_attention_stats: true
log_hyperbolic_stats: true

# Gradient monitoring
log_grad_norms: true
clip_grad_norm: true

# =========================
# Experimental Settings
# =========================
# For ablation studies
euclidean_only: false      # Set true to disable hyperbolic branch
hyperbolic_only: false     # Set true to disable Euclidean branch
progressive_fusion: true   # Gradually increase alpha during training

# Advanced numerical stability
use_identity_attention_fallback: true  # For all-masked rows
enable_attention_debugging: true       # Extra logging for attention issues
safe_mode_steps: 100                  # Extra careful for first N steps