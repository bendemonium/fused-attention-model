model:
  vocab_size: 50257  # GPT2
  max_position_embeddings: 512
  d_model: 768
  num_layers: 12
  num_heads: 12
  dim_feedforward: 3072
  attention_dropout: 0.1
  hidden_dropout: 0.1
  activation: "gelu"
  layer_norm_eps: 1e-12
  initializer_range: 0.02  # Conservative initialization
  use_rope: true  # Use RoPE for better positional understanding
  pad_token_id: 0
  bos_token_id: 101
  eos_token_id: 102
  stable_softmax: true  # Use numerically stable softmax

training:
  num_train_steps: 10000
  warmup_steps: 100
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  
  # Safe training settings
  learning_rate: 1e-4
  weight_decay: 0.01
  adam_eps: 1e-8
  
  # Start conservatively
  batch_size: 8  # Small batch size for stability
  mixed_precision: false  # Start without mixed precision
  safe_mode: true  # Enable additional safety checks

  # Logging and checkpoints
  log_every: 10
  save_every: 1000
  eval_every: 500

output_dir: "./safe_training_run"
wandb_project: "fused-attention-stable"
